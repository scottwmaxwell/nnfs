{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25053b2-d666-4eb2-acff-b211df931cc8",
   "metadata": {},
   "source": [
    "# Chapter 4 Activation Functions\n",
    "\n",
    "Activation functions are used to modify the output of neurons. There are generally two types of activation functions. The first is used in the hidden layers, and the second is used in the output layers.\n",
    "\n",
    "Activation functions help in mapping nonlinear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010cf74-7cf5-4e55-b2e7-866d073e1d06",
   "metadata": {},
   "source": [
    "## The Step Activation Function\n",
    "\n",
    "The step activation function is when the output is either greater than 0 or less than or equal to. It is only when the output is greater than 0 when the neuron fires. This is not used as much because there's no way for the optimization functions to determine how close the neuron was to firing as the output is only 0 or 1.\n",
    "\n",
    "## The Linear Activation Function\n",
    "\n",
    "A Linear function is the equation of a line. This is more commonly used for the last layer's output for a regression model where it outputs a scalar value instead of a classification.\n",
    "\n",
    "## The Sigmoid Activation Function\n",
    "\n",
    "This function is more informative as it returns a range from 0 to 1. That being said, this function is generally not used anymore and is replaced with the Rectified Linear Activation Function.\n",
    "\n",
    "## The Rectified Linear Activation Function\n",
    "\n",
    "This function is actualyl simpler than the sigmoid function. \n",
    "\n",
    "The output is x when x is greater than 0\n",
    "Otherwise the output is 0 when less than or equal to 0.\n",
    "\n",
    "y = { x x > 0\n",
    "    { 0 x <= 0\n",
    "\n",
    "This function is used for speed and efficiency. It's close to being a linear function while also being non-linear. \n",
    "\n",
    "## Why use activation functions?\n",
    "\n",
    "Neural networks are used to fit a nonlinear function, which requires at least two hidden layers that contain an activation function to modify the output.\n",
    "\n",
    "A nonlinear function cannot be represnted accurately by a straight line.\n",
    "\n",
    "## Linear Activation in the Hidden Layers\n",
    "\n",
    "If using a Linear activation function, the output of a neural network will always fit to that linear function.\n",
    "\n",
    "## ReLU Activation in a Pair of Neurons\n",
    "\n",
    "See Book. Shows how adjusting the weights and biases can fit a nonlinear function when using the ReLU activation function with just two neurons.\n",
    "\n",
    "## ReLU Activation in the Hidden Layers\n",
    "\n",
    "See Book. Shows how adjusting the weights and biases can fit a nonlinear function when using the ReLU activation function with a full neural network\n",
    "\n",
    "## ReLU Activation with Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88dba01c-9bd8-4439-a6bf-b791a531124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "output = []\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb7347-56c8-4ca1-bd19-c6979a895771",
   "metadata": {},
   "source": [
    "We can simplify the above even further with using the `max()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49205024-6898-4384-bdae-fdd03e9ef621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266528e-5c13-494e-86bd-b5ab12a0cd7f",
   "metadata": {},
   "source": [
    "Numpy contains an equivalent max() function called maximum(). It compares each element of the input list and returns an object of the same shape filled with new values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18298f71-4b32-483a-bc48-b37b34763cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = np.maximum(0, inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d96d42-2d13-4aea-a036-596d27a42ffc",
   "metadata": {},
   "source": [
    "Applying this function to the dense layer's outputs in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cebb21c8-d7de-4c02-b99a-8cdacc424680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.23085807e-04 9.14717560e-05 0.00000000e+00]\n",
      " [2.00419555e-04 1.79987627e-04 0.00000000e+00]\n",
      " [2.14781301e-04 2.52986393e-04 0.00000000e+00]\n",
      " [4.11037891e-04 3.61288384e-04 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "\n",
    "        # initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights, and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    # Foward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples = 100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU Activation (to be used with dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Forward pass through activation function\n",
    "# Takes in output from previous layer\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# View first few samples:\n",
    "print(activation1.output[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtulENV",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
